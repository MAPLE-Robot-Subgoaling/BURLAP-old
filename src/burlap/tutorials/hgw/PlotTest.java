package burlap.tutorials.hgw;

import burlap.behavior.singleagent.auxiliary.performance.LearningAlgorithmExperimenter;
import burlap.behavior.singleagent.auxiliary.performance.PerformanceMetric;
import burlap.behavior.singleagent.auxiliary.performance.TrialMode;
import burlap.behavior.singleagent.learning.LearningAgent;
import burlap.behavior.singleagent.learning.LearningAgentFactory;
import burlap.behavior.singleagent.learning.tdmethods.QLearning;
import burlap.domain.singleagent.gridworld.GridWorldDomain;
import burlap.oomdp.auxiliary.common.ConstantStateGenerator;
import burlap.oomdp.auxiliary.common.SinglePFTF;
import burlap.oomdp.auxiliary.stateconditiontest.TFGoalCondition;
import burlap.oomdp.core.Domain;
import burlap.oomdp.core.TerminalFunction;
import burlap.oomdp.core.states.State;
import burlap.oomdp.singleagent.RewardFunction;
import burlap.oomdp.singleagent.common.GoalBasedRF;
import burlap.oomdp.singleagent.environment.SimulatedEnvironment;
import burlap.oomdp.statehashing.SimpleHashableStateFactory;

public class PlotTest {

	public static void main(String[] args) {

		GridWorldDomain gw = new GridWorldDomain(11, 11); // 11x11 grid world
		gw.setMapToFourRooms(); // four rooms layout
		gw.setProbSucceedTransitionDynamics(0.8); // stochastic transitions with
													// 0.8 success rate
		final Domain domain = gw.generateDomain(); // generate the grid world
													// domain

		// setup initial state
		State s = GridWorldDomain.getOneAgentOneLocationState(domain);
		GridWorldDomain.setAgent(s, 0, 0);
		GridWorldDomain.setLocation(s, 0, 10, 10);

		// ends when the agent reaches a location
		final TerminalFunction tf = new SinglePFTF(
				domain.getPropFunction(GridWorldDomain.PFATLOCATION));

		// reward function definition
		final RewardFunction rf = new GoalBasedRF(new TFGoalCondition(tf), 5.,
				-0.1);

		// initial state generator
		final ConstantStateGenerator sg = new ConstantStateGenerator(s);

		// set up the state hashing system for looking up states
		final SimpleHashableStateFactory hashingFactory = new SimpleHashableStateFactory();

		/**
		 * Create factory for Q-learning agent
		 */
		LearningAgentFactory qLearningFactory = new LearningAgentFactory() {

			@Override
			public String getAgentName() {
				return "Q-learning";
			}

			@Override
			public LearningAgent generateAgent() {
				return new QLearning(domain, 0.99, hashingFactory, 0.3, 0.1);
			}
		};

		// define learning environment
		SimulatedEnvironment env = new SimulatedEnvironment(domain, rf, tf, sg);

		// define experiment
		LearningAlgorithmExperimenter exp = new LearningAlgorithmExperimenter(
				env, 10, 100, qLearningFactory);

		exp.setUpPlottingConfiguration(500, 250, 2, 1000,
				TrialMode.MOSTRECENTANDAVERAGE,
				PerformanceMetric.CUMULATIVESTEPSPEREPISODE,
				PerformanceMetric.AVERAGEEPISODEREWARD);

		// start experiment
		exp.startExperiment();

	}

}
